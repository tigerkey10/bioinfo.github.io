<!DOCTYPE html><html lang="ko-KR" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="공부한 내용을 기록한 글" /><meta property="og:description" content="공부한 내용을 기록한 글" /><link rel="canonical" href="https://kibeomkim.me/posts/machine_learning_basic/" /><meta property="og:url" content="https://kibeomkim.me/posts/machine_learning_basic/" /><meta property="og:site_name" content="der Wille zur Macht" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-04T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름" /><meta name="google-site-verification" content="1CLBUlwAX10LXrW4RzkPuaeWtey34c3dyngs14cKrCw" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-06T00:00:00+09:00","datePublished":"2022-02-04T00:00:00+09:00","description":"공부한 내용을 기록한 글","headline":"[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름","mainEntityOfPage":{"@type":"WebPage","@id":"https://kibeomkim.me/posts/machine_learning_basic/"},"url":"https://kibeomkim.me/posts/machine_learning_basic/"}</script><title>[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름 | der Wille zur Macht</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="der Wille zur Macht"><meta name="application-name" content="der Wille zur Macht"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/emoge.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">der Wille zur Macht</a></div><div class="site-subtitle font-italic">10,000 Hours</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>카테고리</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>태그</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>타임라인</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a> <a href="https://github.com/tigerkey10" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['kkb1004tom','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 포스트</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="검색..."> </span> <span id="search-cancel" >취소</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>[Keras/딥러닝 공부] 머신러닝 기법 분류, 데이터셋 분리 기법, 데이터 전처리 기법, 규제 기법, 머신러닝 작업 흐름</h1><div class="post-meta text-muted"> <span> 게시 <em class="" data-ts="1643900400" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-02-04 </em> </span> <span> 업데이트 <em class="" data-ts="1644105600" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-02-06 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/tigerkey10/bioinfo.github.io">Ki Beom Kim</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6538 단어"> <em>36 분</em>읽는 시간</span></div></div></div><div class="post-content"><p>아래 내용은 ‘케라스 창시자에게 배우는 딥러닝 (프랑소와 슐레 저, 박해선 옮김, 길벗 출판사)’ 을 공부한 뒤, 배운 내용을 제 언어로 정리.기록한 것 입니다.</p><hr /><h1 id="머신러닝-네-가지-분류">머신러닝 네 가지 분류</h1><h2 id="지도-학습"><span class="mr-2">지도 학습</span><a href="#지도-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의"><span class="mr-2">정의</span><a href="#정의" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>사람이 정답 주고, 모델이 주어진 정답 잘 맞추도록 학습시키는 기법</p><h3 id="종류"><span class="mr-2">종류</span><a href="#종류" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>분류<li>회귀<li>시퀀스 생성<li>구문 트리 예측<li>물체 감지<li>이미지 분할</ul><hr /><h2 id="비지도-학습"><span class="mr-2">비지도 학습</span><a href="#비지도-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-1"><span class="mr-2">정의</span><a href="#정의-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>데이터 자체의 특성 파악. 추출하도록 학습시키는 기법</p><h3 id="종류-1"><span class="mr-2">종류</span><a href="#종류-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>차원 축소<li>군집(Clustering)</ul><hr /><h2 id="자기-지도-학습"><span class="mr-2">자기 지도 학습</span><a href="#자기-지도-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-2"><span class="mr-2">정의</span><a href="#정의-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>사람이 개입하지 않는 지도학습</p><ul><li>모형이 경험적 알고리듬(heuristic algorithm) 사용해 입력 데이터로부터 레이블 생성한다.</ul><h3 id="종류-2"><span class="mr-2">종류</span><a href="#종류-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>오토인코더(autoencoder)<li>시간에 따른 지도 학습</ul><hr /><h2 id="강화학습"><span class="mr-2">강화학습</span><a href="#강화학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h2 id="정의-3"><span class="mr-2">정의</span><a href="#정의-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모형이 주어진 상황에서 보상 강화하는 출력을 선택하도록 학습시키는 기법</p><ul><li>아직 현장에서는 잘 사용되지 않고, 연구 영역에 있다.<li>하지만 발전 가능성, 비전 있는 머신러닝 분야다.</ul><h2 id="예"><span class="mr-2">예</span><a href="#예" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>게임에서 강화학습</p><ul><li>게임 속 전장 상황이 주어지면, 모형이 게임 점수 최대화 할 수 있는 게임 내 행동을 출력</ul><hr /><h1 id="머신러닝-모델-평가">머신러닝 모델 평가</h1><p>‘일반화 할 수 있는 모델 평가’</p><p>$\Rightarrow$ 새 데이터셋에서 모델 성능 평가</p><ul><li>모델 성능 평가 위해 ‘신뢰할 수 있는 모델 성능 측정 방법’이 필요하다.</ul><hr /><h1 id="데이터셋을-훈련용-검증용-테스트용-셋으로-나누기">데이터셋을 훈련용, 검증용, 테스트용 셋으로 나누기</h1><h2 id="모델-훈련시키고-성능-평가하는-과정"><span class="mr-2">모델 훈련시키고 성능 평가하는 과정</span><a href="#모델-훈련시키고-성능-평가하는-과정" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>훈련용 셋에서 모델을 훈련시킨다.<li>검증용 셋으로 새 데이터셋에서 모델 성능 평가한다.<li>검증용 셋 결과 가지고. 신경망 각 층의 히든유닛 수, 층 수 등 하이퍼파라미터 ‘튜닝’한다. 모델 성능 올리기 위한 작업이다.<li>튜닝된 모델을 다시 훈련용 셋으로 훈련시키고, 성능 검증하고, 튜닝한다.<li>튜닝 모두 끝나면 테스트용 셋으로 딱 한 번 모델 성능 평가한다.</ul><h2 id="테스트용-셋-따로-두는-이유"><span class="mr-2">테스트용 셋 따로 두는 이유</span><a href="#테스트용-셋-따로-두는-이유" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>과정 중 하이퍼파라미터 튜닝 반복하면서 모델이 검증용 셋에 과적합 되는 경향 나타난다.</p><p>따라서 튜닝 반복하다 보면 검증용 셋에서 성능이 갈수록 올라갈 수 밖에 없다.</p><p>모델 일반화 성능 제대로 평가하기 위해 완전히 새로운 데이터가 필요하고, 그 역할 하는 게 테스트 셋이다.</p><h1 id="데이터셋-나누는-기법">데이터셋 나누는 기법</h1><h2 id="단순-홀드아웃-검증"><span class="mr-2">단순 홀드아웃 검증</span><a href="#단순-홀드아웃-검증" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Hold out: 남겨두다</p><h2 id="정의-4"><span class="mr-2">정의</span><a href="#정의-4" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>전체 셋에서 검증용 셋 따로 떼어두는 방법</p><ul><li>테스트용 셋은 검증용 셋 분리 전에 따로 떼 두었다고 가정</ul><p><img width="519" alt="Screen Shot 2022-02-04 at 11 25 50" data-src="https://user-images.githubusercontent.com/83487073/152462213-cb66331b-58d2-4529-a5ac-aef2d5ca2f77.png" data-proofer-ignore></p><ul><li>훈련용 셋으로 모델 훈련시킨다<li>검증 셋으로 모델 성능 검증하고, 하이퍼파라미터 튜닝한다</ul><h2 id="장점"><span class="mr-2">장점</span><a href="#장점" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>단순하다. 복잡한 작업 필요 없다.</p><h2 id="단점"><span class="mr-2">단점</span><a href="#단점" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>데이터 적으면 훈련용 셋과 검증용 셋의 전체 데이터에 대한 통계적 대표성 떨어진다.</p><p>$\Rightarrow$ 데이터 수 적을 때는 적용할 수 없다.</p><h2 id="k-겹-교차검증"><span class="mr-2">K-겹 교차검증</span><a href="#k-겹-교차검증" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>데이터 수 작을 때 특히 유용한 방법이다.</p><h2 id="정의-5"><span class="mr-2">정의</span><a href="#정의-5" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>전체 데이터셋 k개 분할로 나눠 그 중 하나는 검증용 셋, 나머지는 훈련용 셋으로 삼는 방법</p><ul><li>모델 훈련 - 검증 과정 k번 반복한다.<li>k개 성능 점수 평균을 최종 성능 점수로 삼는다.</ul><p><img width="934" alt="Screen Shot 2022-02-04 at 12 32 58" data-src="https://user-images.githubusercontent.com/83487073/152467557-0862f6ed-0e54-453d-a210-a4c68d91d0cc.png" data-proofer-ignore></p><h2 id="셔플링-사용한-반복-k-겹-교차검증"><span class="mr-2">셔플링 사용한 반복 K-겹 교차검증</span><a href="#셔플링-사용한-반복-k-겹-교차검증" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>데이터 수 작을 때 특히 유용한 방법이다.</p><h2 id="정의-6"><span class="mr-2">정의</span><a href="#정의-6" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>P번 반복해서 K-겹 교차검증 수행</p><ul><li>K-겹 교차검증 수행 하기 전 매번 데이터셋 무작위로 섞는다(셔플)<li>P번의 K-겹 교차검증 점수 평균이 최종 점수 된다.</ul><h2 id="단점-1"><span class="mr-2">단점</span><a href="#단점-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>시간 많이 걸린다.</p><hr /><h1 id="이외-기억해야-할-점">이외 기억해야 할 점</h1><ul><li>데이터셋을 훈련용 셋, 검증용 셋, 테스트 셋으로 나누기 전에 되도록 데이터셋 한번 섞자(셔플).</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="복사되었습니다!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="c1"># 셔플 
</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></table></code></div></div><ul><li><p>데이터에 시간 순서가 나타나면 절대 섞으면 안 된다. 훈련용 셋은 상대적으로 과거 데이터, 테스트 셋은 상대적으로 미래 데이터로 구성되도록 분리하자.</p><li><p>데이터셋에 중복된 데이터(레코드)가 있으면 제거하는 것이 좋다.</p></ul><hr /><h1 id="데이터-전처리">데이터 전처리</h1><p>데이터 전처리 기법들은 입력 데이터 종류별로 특화되어 있다. 예컨대 이미지, 텍스트 데이터 전처리 방법이 다르다.</p><h1 id="신경망-위한-데이터-전처리-일반론">신경망 위한 데이터 전처리 일반론</h1><h2 id="벡터화데이터-벡터화"><span class="mr-2">벡터화(데이터 벡터화)</span><a href="#벡터화데이터-벡터화" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-7"><span class="mr-2">정의</span><a href="#정의-7" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>입력 데이터를 부동 소수점 실수 또는 정수로 구성된 텐서로 변환하는 작업.</p><ul><li>신경망 모든 입력은 텐서여야 하므로, 데이터 전처리 할 때 반드시 거치는 과정이다.</ul><hr /><h2 id="정규화"><span class="mr-2">정규화</span><a href="#정규화" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-8"><span class="mr-2">정의</span><a href="#정의-8" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>각 데이터를 0과 1 사이(또는 작은 값) 로 변환하고, 특성값들 간 스케일 맞춰주는 작업이다.</p><ul><li>신경망의 원활한 학습 위해 반드시 거쳐야 할 과정이다.</ul><h2 id="보다-엄격한-정규화수학적-정규화"><span class="mr-2">보다 엄격한 정규화(수학적 정규화)</span><a href="#보다-엄격한-정규화수학적-정규화" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-9"><span class="mr-2">정의</span><a href="#정의-9" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>각 특성 별 데이터를 평균이 0, 표준편차가 1로 만드는 작업.</p><hr /><h2 id="누락된-값nullna-값-다루기"><span class="mr-2">누락된 값(Null/NA) 값 다루기</span><a href="#누락된-값nullna-값-다루기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>전체 평균에 영향 미치지 않는 값으로 누락된 값 채운다.</p><h2 id="null-자리에-뭘-넣는가"><span class="mr-2">Null 자리에 뭘 넣는가</span><a href="#null-자리에-뭘-넣는가" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>일반적으로 0 넣는다.<li>평균값을 넣기도 한다.<li>중앙값을 넣기도 한다.</ul><p>만약 훈련용 셋 누락 값을 그 평균. 중앙값으로 대체하기로 했다면,</p><p>테스트 셋 누락 값도 훈련용 셋 평균. 중앙값으로 대체해야 한다.</p><p>교차검증 할 때도 검증용 셋 누락 값은 훈련용 셋 평균. 중앙값으로 채워야 한다.</p><h2 id="만약-훈련용-셋에-누락-값-없는데-테스트셋에-있다면"><span class="mr-2">만약 훈련용 셋에 누락 값 없는데 테스트셋에 있다면?</span><a href="#만약-훈련용-셋에-누락-값-없는데-테스트셋에-있다면" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델이 훈련 받을 때 누락 값 처리 방법을 학습하지 못했으므로, 문제 발생한다.</p><h2 id="따라서"><span class="mr-2">따라서</span><a href="#따라서" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>전체 데이터셋에서 누락 값 있는 샘플(레코드 or 행벡터) 수가 적다면, 테스트셋 떼어놓기 전 이 레코드들 제외한다.<li>누락된 값 있는 특성이 별로 안 중요하면, 이 특성을 통째로 제외하고 테스트셋 떼어 놓는다.</ul><hr /><h2 id="특성-공학"><span class="mr-2">특성 공학</span><a href="#특성-공학" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정의-10"><span class="mr-2">정의</span><a href="#정의-10" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>원본 데이터에서 특성만 추출해서 데이터 변환하는 작업.</p><ul><li>문제에 대한 명료한 정의를 내릴 수 있어야 한다.</ul><h3 id="예시"><span class="mr-2">예시</span><a href="#예시" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>시계 사진(이미지) 에 나타난 시간 정보 출력하는 모형 만들고 싶다.</p><ul><li>시계 사진 그대로 써서 정보 추출하기에는 보다 복잡한 모형, 높은 컴퓨팅 파워 필요하다.</ul><p>한편 시계 사진 데이터에 특성 공학 적용하면 아래와 같아진다.</p><p>내가 추출하고 싶은 정보는 ‘시간 정보’다.</p><p>그러면 굳이 원본 데이터 전체가 필요 없다. 시간 정보만 있으면 된다.</p><h3 id="rightarrow-원본-데이터에서-시간-정보-나타내는-특성만-추출한다"><span class="mr-2">$\Rightarrow$ 원본 데이터에서 시간 정보 나타내는 특성만 추출한다.</span><a href="#rightarrow-원본-데이터에서-시간-정보-나타내는-특성만-추출한다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>여기서 ‘시간’에 대한 정의가 필요하다.</p><h3 id="시간-정의-초침과-분침이-가리키는-지점"><span class="mr-2">시간 정의: 초침과 분침이 가리키는 지점.</span><a href="#시간-정의-초침과-분침이-가리키는-지점" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>$\Rightarrow$ 원본 데이터에서 초침과 분침이 가리키는 지점 정보만 추출한다.</p><p>또는</p><h3 id="시간-정의-초침과-분침이-이루는-각도"><span class="mr-2">시간 정의: 초침과 분침이 이루는 각도.</span><a href="#시간-정의-초침과-분침이-이루는-각도" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>$\Rightarrow$ 원본 데이터에서 초침과 분침이 이루는 각도 정보만 추출한다.</p><h3 id="결과로"><span class="mr-2">결과로</span><a href="#결과로" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>2차원 벡터공간 상의 특정 지점(point)</p><p>또는</p><p>원점과 2차원 직교좌표계를 중심으로 한 어떤 각도 값들로</p><p>구성된 1차원 텐서(벡터)가 나올 것이다.</p><p>이 벡터가 원본 이미지 데이터가 ‘변환된’ 데이터 이고, 이렇게 원본 데이터 변환하는 작업을 ‘특성 공학’ 이라 한다.</p><h3 id="쓰임"><span class="mr-2">쓰임</span><a href="#쓰임" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>특성 공학은 전통적 머신러닝 기법들 사용할 때 아주 중요하게 쓰인다.<li>딥러닝 기법 사용할 때는 특성 공학 필요 없다.</ul><h3 id="그럼에도"><span class="mr-2">그럼에도</span><a href="#그럼에도" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>특성 공학 사용하면. 딥러닝 모델 썼을 때 보다. 특정 문제를 더 적은 자원 &amp; 훨씬 효율적으로 해결할 수 있다. 위 시계 문제가 예다.<li>데이터 수 적어서 딥러닝 모델 적용할 수 없을 때. 특성 공학 사용하면 적은 데이터로 문제 효과적으로 해결할 수 있다.</ul><hr /><h1 id="과대적합과-과소적합">과대적합과 과소적합</h1><h2 id="머신러닝-근본-이슈는-일반화와-최적화-사이-줄다리기"><span class="mr-2">머신러닝 근본 이슈는 ‘일반화’와 ‘최적화’ 사이 줄다리기</span><a href="#머신러닝-근본-이슈는-일반화와-최적화-사이-줄다리기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>최적화는 ‘훈련 데이터’에서 모델 성능 최대화 하기 위해 최적 파라미터 찾는 작업 말한다.</p><p>일반화는 ‘새 데이터’에서 모델 성능이 잘 나오도록 하는 걸 말한다.</p><p>최적화가 과도하면 과대적합 나타난다. 모델 일반화 성능은 떨어진다.</p><p>반면 최적화 부족하면 과소적합 나타난다. 모델 일반화 성능 더 끌어올릴 여지 남아있다.</p><h2 id="과대적합overfitting"><span class="mr-2">과대적합(Overfitting)</span><a href="#과대적합overfitting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모든 머신러닝 문제에서 과대적합은 종종.자주. 마주치는 문제다.</p><p>따라서 머신러닝에서는 과대적합 잘 제어하는 것이 중요하다.</p><h2 id="정의-11"><span class="mr-2">정의</span><a href="#정의-11" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델이 학습 데이터에 특화된 패턴을 학습하기 시작한 상태.</p><h3 id="rightarrow-모델이-학습-데이터와-레이블을-외워버리기-시작한-상태"><span class="mr-2">$\Rightarrow$ 모델이 학습 데이터와 레이블을 ‘외워버리기 시작한’ 상태.</span><a href="#rightarrow-모델이-학습-데이터와-레이블을-외워버리기-시작한-상태" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h2 id="과대적합-있을-때-모델-성능"><span class="mr-2">과대적합 있을 때 모델 성능</span><a href="#과대적합-있을-때-모델-성능" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>과대적합이 나타나면 검증용 셋에서 모델 성능은 떨어지기 시작한다.</p><p>테스트 셋에서도 모델 성능이 낮게 나온다. 곧, 과대적합 나타나면 모델 일반화 성능 떨어진다.</p><h2 id="과소적합underfitting"><span class="mr-2">과소적합(Underfitting)</span><a href="#과소적합underfitting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델 훈련 초기에 나타난다.</p><h2 id="정의-12"><span class="mr-2">정의</span><a href="#정의-12" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델이 훈련 데이터에 나타난 특징들을 아직 충분히(모두) 학습하지 못한 상태.</p><h2 id="과소적합-있을-때-모델-성능"><span class="mr-2">과소적합 있을 때 모델 성능</span><a href="#과소적합-있을-때-모델-성능" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>과소적합 있을 때, 모델 성능은 훈련용 셋과 검증용 셋 모두에서 함께 증가한다.</p><p>과소적합 있을 때는 모델 성능이 아직 더 향상될 여지가 남아있다.</p><ul><li>과소적합 상태 끝나고나면 곧이어 과대적합 나타나기 시작한다.</ul><hr /><h1 id="규제regularization">규제(Regularization)</h1><h2 id="정의-13"><span class="mr-2">정의</span><a href="#정의-13" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델에 과대적합 발생 억제하는 과정</p><ul><li>모델에 과대적합 발생하면 모델 일반화 성능(모델 개발 목표)이 떨어진다. 그래서 규제 통해 과대적합 발생 억제한다.</ul><h2 id="종류-3"><span class="mr-2">종류</span><a href="#종류-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h2 id="더-많은-훈련-데이터-모으기"><span class="mr-2">더 많은 훈련 데이터 모으기</span><a href="#더-많은-훈련-데이터-모으기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>과대적합 억제하는 가장 좋은 방법이다. 훈련 데이터가 많으면 많을 수록. 과대적합 발생 억제되고, 모델 일반화 성능도 올라간다.</p><h2 id="네트워크-크기-축소모델-학습-파라미터-수-줄이기"><span class="mr-2">네트워크 크기 축소(모델 학습 파라미터 수 줄이기)</span><a href="#네트워크-크기-축소모델-학습-파라미터-수-줄이기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>*학습 파라미터 수 = 모델 크기 = 모델 용량</p><p>모델 학습 파라미터 수를 줄인다는 건. 모델이 제한적 정보만 저장할 수 있도록 한다는 거다.</p><p>모델이 제한적 정보만 저장하게 되면. 보다 중요한 패턴에 집중하게 된다.</p><p>이렇게 해서 모델 일반화 성능을 끌어올릴 수 있다.</p><p>(딥러닝 모델은 항상 과대적합 쪽으로 흘려가려는 경향이 있다. 즉, 놔두면 과도한 최적화 쪽으로 알아서 흘러간다는 거다. 따라서 우리의 관심사는 ‘일반화’다)</p><p>다만 유념해야 할 것은. 모델 학습 파라미터 수 너무 줄이면 과소적합 발생한다는 거다.</p><p>따라서 과대적합 피하기 위해 학습 파라미터 수를 줄이되, 적정한 정도로 줄이는 것이 좋다.</p><h3 id="학습-파라미터-수는-층-수-또는-각-층-히든유닛-수-줄이면-감소된다"><span class="mr-2">학습 파라미터 수는 층 수 또는 각 층 히든유닛 수 줄이면 감소된다.</span><a href="#학습-파라미터-수는-층-수-또는-각-층-히든유닛-수-줄이면-감소된다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h2 id="가중치-규제-추가"><span class="mr-2">가중치 규제 추가</span><a href="#가중치-규제-추가" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>가중치 ‘크기’ 규제.</p><p>정의: 가중치가 작은 값만 갖도록 규제하는 작업이다.</p><p>방법: 손실함수에 가중치 크기만큼 손실(비용) 추가한다.</p><p>가중치 규제 종류:</p><p>L1규제: 가중치 벡터 요소들의 절댓값에 비례하는 비용을 손실함수에 추가한다(가중치 L1 놈(norm))</p><p>$\Rightarrow$ 손실함수 + 비용(상숫값)</p><p>L2규제: 가중치 벡터 놈 제곱을 손실함수에 추가한다(가중치 L2 놈(norm)). 가중치 감쇠(weight decay) 라고도 한다.</p><p>$\Rightarrow$ 손실함수 + 가중치 크기(상숫값)</p><ul><li>케라스에서 모형에 가중치 규제 적용하려면 각 층 kernel_regularizer 매개변수에 가중치 규제 객체 전달하면 된다.</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="복사되었습니다!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># 모델에 가중치 규제 적용 예 
</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span> 

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span> 

<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>

<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="p">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
</pre></table></code></div></div><ul><li>l2(0.001)은 가중치 행렬 요소 제곱 합 값에 0.001 곱한 결과값을 손실함수에 더한다는 뜻이다. 이 결과값 항을 ‘패널티 항’ 이라고도 한다. 패널티 항은 훈련할 때만 손실함수에 추가된다.<li>훈련할 때 손실함수에 패널티 항이 추가되면서. 훈련 손실은 추가 전보다 높아질 것이다.<li>한편 손실함수 최적화 하면서 가중치 크기도 자연스레 함께 줄어들 것이다. 가중치 크기 작아지면 과대적합 완화할 수 있다. 훈련 완료된 모델은 과대적합에 잘 견딜 것이다. 따라서 모델을 검증용 데이터로 성능 테스트하면. 검증 손실 그래프 기울기가 훨씬 완만하게 증가할 것이다. 곧, 모델 일반화 성능도 보다 높일 수 있다.</ul><p>l2 규제 이외 l1규제 또는 l1, l2규제 동시 적용 위해 아래 코드를 적용할 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="복사되었습니다!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="c1"># l1규제, l1,l2규제 동시 적용 
</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span>

<span class="n">regularizers</span><span class="p">.</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># l1규제 
</span>
<span class="n">regularizers</span><span class="p">.</span><span class="n">l1_l2</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># l1, l2규제 동시 적용 
</span></pre></table></code></div></div><h2 id="드롭아웃-추가"><span class="mr-2">드롭아웃 추가</span><a href="#드롭아웃-추가" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="드롭아웃-정의"><span class="mr-2">드롭아웃 정의</span><a href="#드롭아웃-정의" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>훈련 동안. 층의 출력 특성 중 일부를. 무작위로. 0 만드는 규제기법.</p><h3 id="특징"><span class="mr-2">특징</span><a href="#특징" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>각 층에 적용한다.<li>가장 효과적이고 널리 사용되는 규제기법이다.<li>훈련 동안만 적용한다.</ul><h3 id="드롭아웃-비율"><span class="mr-2">드롭아웃 비율</span><a href="#드롭아웃-비율" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>층 출력의 요소 중 몇% 를 0으로 만들지 나타내는 비율이다.</p><p>예컨대 드롭아웃 비율이 0.5 면 층 출력 요소 중 절반을 랜덤하게 0 만든다.</p><p>층 출력이 $[0.5, 0.2, 0.4, 0.6]^{T}$ 이라고 하면. 0.5 비율로 드롭아웃 적용했을 때 $[0, 0.2, 0.4, 0]^{T}$ 으로 바뀌는 식이다.</p><ul><li>테스트 단계에서는 드롭아웃 적용하지 않는다. 대신, 각 층 출력의 요소들을 드롭아웃 비율만큼 스케일 다운 해야 한다. 예컨대 드롭아웃 비율이 0.5 였으면, 테스트 단계 각 층 출력 요소들에 0.5씩 곱해서 스케일을 절반으로 줄인다. 테스트 단계 층 출력이 $[1,2,3,4]^{T}$ 면, 각 요소에 0.5 씩 곱해서 $[0.5, 1, 1.5, 2]^{T}$ 로 스케일 낮추는 식이다.</ul><h3 id="테스트-단계-출력-스케일-유지하는-다른-방법"><span class="mr-2">테스트 단계 출력 스케일 유지하는 다른 방법</span><a href="#테스트-단계-출력-스케일-유지하는-다른-방법" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>훈련 단계 층 출력에 드롭아웃 적용하고, 출력 각 요소들을 드롭아웃 비율만큼 역으로 스케일 업 시킨다.</p><p>예컨대 드롭아웃 비율이 $0.5$ 였으면. 층 출력 각 요소들 스케일 $2$ 배로 키운다.</p><p>$[1,2,3,4]^{T} \Rightarrow [2,4,6,8]^{T}$</p><p>이러면 테스트 단계 층 출력은 스케일 변화시킬 필요 없다.</p><h3 id="드롭아웃-규제기법이-과대적합-감소시키는-원리"><span class="mr-2">드롭아웃 규제기법이 과대적합 감소시키는 원리</span><a href="#드롭아웃-규제기법이-과대적합-감소시키는-원리" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h3 id="층-출력에-노이즈-추가0-해서-훈련-데이터에-특화된-지엽적-패턴을-깨뜨린다"><span class="mr-2">층 출력에 노이즈 추가($0$) 해서. 훈련 데이터에 특화된 지엽적 패턴을 깨뜨린다.</span><a href="#층-출력에-노이즈-추가0-해서-훈련-데이터에-특화된-지엽적-패턴을-깨뜨린다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>결과로 훈련 동안 모델이 지엽적 패턴을 학습하지 못하게 되고, 훈련 데이터의 주요 패턴(특성)만 집중적으로 학습하게 될 것이다. 이는 과대적합 회피로 이어진다.</p><h3 id="케라스에서-드롭아웃-적용"><span class="mr-2">케라스에서 드롭아웃 적용</span><a href="#케라스에서-드롭아웃-적용" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>각 층 바로 다음에 드롭아웃 층을 배치하는 방식으로 각 층에 드롭아웃 적용할 수 있다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="복사되었습니다!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># 모델 각 층에 드롭아웃 적용 
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span> 
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,)))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span> <span class="c1"># 드롭아웃 비율 = 0.5
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span> <span class="c1"># 드롭아웃 비율 = 0.5
</span><span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
</pre></table></code></div></div><hr /><h1 id="보편적-머신러닝-작업-흐름">보편적 머신러닝 작업 흐름</h1><h2 id="1-문제-정의--모델-학습시킬-데이터-수집"><span class="mr-2">1. 문제 정의 &amp; 모델 학습시킬 데이터 수집</span><a href="#1-문제-정의--모델-학습시킬-데이터-수집" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>내가 지금 해결하려는 문제가 이진 분류인지, 다중 분류인지, 회귀인지 파악해야 한다.</p><p>동시에 지금 문제 해결을 위해 필요한 데이터는 무엇인지, 그 데이터를 구할 수 있는지 등도 따져봐야 한다.</p><h2 id="2-모델-성능-측정-지표-설정하기--손실함수-선택하기"><span class="mr-2">2. 모델 성능 측정 지표 설정하기 &amp; 손실함수 선택하기</span><a href="#2-모델-성능-측정-지표-설정하기--손실함수-선택하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델 성능 측정 지표가 손실함수 선택 기준이 된다.</p><h2 id="3-모델-성능-검증-방법-설정하기데이터셋-나누기"><span class="mr-2">3. 모델 성능 검증 방법 설정하기(데이터셋 나누기)</span><a href="#3-모델-성능-검증-방법-설정하기데이터셋-나누기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>단순 홀드아웃 검증, K-겹 교차검증, 반복 K-겹 교차검증 중 한 방법을 선택해 전체 데이터셋을 훈련용 셋과 검증용 셋으로 분리한다.</p><p>단순 홀드아웃 검증 - 전체 데이터셋을 훈련용 셋과 검증용 셋으로 나눈 뒤 검증용 셋으로 일반화 성능 검증한다.</p><p>K-겹 교차검증 - 전체 셋을 K개로 나눈 뒤 그 중 하나는 검증용 셋, 나머지는 훈련용 셋으로 사용한다. K번 과정 반복한다.</p><p>반복 K-겹 교차검증 - K-겹 교차검증을 P번 반복한다. 단, K-겹 교차검증 매번 시행하기 전에. 셔플(Shuffle) 통해 데이터셋을 함 섞는다.</p><p>대부분 경우 단순 홀드아웃 검증이면 충분하다. 데이터가 충분할 것이기 때문이다.</p><p>하지만 데이터가 부족한 경우엔 K-겹 교차검증 또는 반복 K-겹 교차검증이 유용한 대안이다.</p><h2 id="4-데이터-전처리"><span class="mr-2">4. 데이터 전처리</span><a href="#4-데이터-전처리" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li><p>신경망의 입력은 텐서다. 텐서는 넘파이 다차원 배열을 일컫는다. 따라서 신경망에 데이터 주입 전, 모든 데이터를 부동 소수점 실수 또는 정수 텐서로 바꿔줘야 한다.</p><li><p>각 특성값들은 모두 스케일이 비슷해지도록 조정해야 한다. 대표적 방법으로 정규화가 있다.</p><li><p>데이터 수가 적어서 신경망 적용이 어렵거나, 굳이 신경망 안 써도 될 문제라면 특성 공학 써서 보다 효율적으로 해결할 수도 있다.</p></ul><h2 id="5-크기-작은-모델로-시작하기"><span class="mr-2">5. 크기 작은 모델로 시작하기</span><a href="#5-크기-작은-모델로-시작하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="과소적합이-있는-모델-또는-과도한-일반화-쪽에-가까운-모델"><span class="mr-2">과소적합이 있는 모델. 또는 과도한 일반화 쪽에 가까운 모델.</span><a href="#과소적합이-있는-모델-또는-과도한-일반화-쪽에-가까운-모델" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>층 수 또는 히든유닛 수 작은 모델로 적합한 모델 찾는 과정 시작한다.</p><p>이런 모델을 통계적 검정력이 확보된 모델이라고도 한다.</p><p>모델 만들 때 세 가지 요소를 선택한다.</p><ul><li><p>마지막 층 활성화 함수: 신경망 출력 값에 필요한 제한을 가한다. 이진분류 예로 들면, 마지막 출력 층 활성화 함수로 시그모이드 함수가 들어가 출력 값을 0과 1 사이로 제한해준다.</p><li><p>손실함수: 풀려는 문제에 적합한 손실함수를 선택한다. 예컨대 회귀문제라면 손실함수로 $mse$ 를 선택할 것이고, 이진분류 문제라면 binary crossentropy (로그손실) 를 사용할 것이다.</p><li><p>최적화 알고리듬 설정: 옵티마이저와 학습률 선택한다. 일반적으로 확률적 경사 하강법(rmsprop)과 기본 학습률 사용한다.</p></ul><h2 id="6-모델-몸집-키우기-과대적합-모델-구축"><span class="mr-2">6. 모델 몸집 키우기: 과대적합 모델 구축</span><a href="#6-모델-몸집-키우기-과대적합-모델-구축" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>통계적 검정력이 확보된 모델은 크기가 작아서 과소적합 여지가 있다.</p><p>곧, 성능이 더 향상될 여지가 남아있다.</p><p>머신러닝을 한줄로 정의히자면 ‘일반화와 최적화 사이 줄다리기’ 로 정의할 수 있다.</p><p>현재 모델 상태는 ‘과도한 일반화’ 쪽에 가깝다. 최적화 쪽으로 옮겨서 둘 사이의 균형점에 도달해야 한다.</p><p><img width="395" alt="Screen Shot 2022-02-06 at 17 01 59" data-src="https://user-images.githubusercontent.com/83487073/152672409-01486d76-d6d0-4ca2-a2cf-673cf03cae29.png" data-proofer-ignore></p><p>하지만 실제로 과소적합과 과대적합 사이 적절한 균형점이 어디인지는 알 수 없다.</p><p>따라서 균형점에 도달하기 위해 일단 균형점을 지나 과도한 최적화(과대적합) 쪽으로 움직인다.</p><p><img width="407" alt="Screen Shot 2022-02-06 at 17 04 09" data-src="https://user-images.githubusercontent.com/83487073/152672469-2eb662c7-487e-43dc-b821-cb3c4c33d97c.png" data-proofer-ignore></p><p>이를 위해 기존 모델 크기를 키운다. 곧, 과대적합 모델을 구축한다.</p><ul><li><p>모델에 층 추가한다.</p><li><p>층 크기를 키운다(히든유닛 수 증가).</p><li><p>더 많은 에포크 동안 훈련한다.</p></ul><p>크기 많이 키울 수록 매우 빠르게 과대적합 도달할 것이다.</p><p>모델 검증 손실이 증가하기 시작하는 지점부터 과대적합에 도달한 것이다.</p><p>이제 모델 규제와 하이퍼파라미터 튜닝 통해 과대적합을 억제하면서. 과소적합과 과대적합 사이 균형점을 찾을 것이다.</p><h2 id="7-모델-규제와-하이퍼파라미터-튜닝"><span class="mr-2">7. 모델 규제와 하이퍼파라미터 튜닝</span><a href="#7-모델-규제와-하이퍼파라미터-튜닝" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>반복적으로 모델을 ‘수정’ 하고 ‘훈련’하고 검증데이터에서 ‘평가’한다. 좋은 모델을 얻을 때 까지 반복한다.</p><p>위에서 모델 규제 방법들을 열거했다.</p><ul><li>훈련 데이터 수 증가시키기<li>네트워크 크기 축소(학습 파라미터 수 줄이기)<li>가중치 크기 규제(가중치 크기가 작도록)<li>드롭아웃 추가</ul><p>한편 층 수, 히든유닛 수, 옵티마이저 학습률 등을 조정하는 걸 하이퍼파라미터 튜닝 이라고 정의했다.</p><p>과대적합된 모델의 검증 결과를 바탕으로 위 규제와 튜닝을 적용한다. 그리고 모델을 훈련데이터로 다시 훈련시킨다.</p><p>훈련된 모델을 검증 데이터로 다시 검증하고, 그 결과를 통해 다시 규제하고 튜닝한다.</p><ul><li>주의점: 모델 수정을 반복할 때 마다 모델이 서서히 검증 데이터에 익숙해진다. 검증 데이터로 모델을 학습시키지도 않았지만 모델이 검증 데이터에 ‘과대적합’ 될 수 있다. 따라서 모델 튜닝을 너무 많이 반복하는 건 검증 데이터 신뢰성을 떨어뜨린다.</ul><p>과정을 반복하면서 만족할 만 한 모델을 얻었다면, 훈련용 데이터와 검증용 데이터를 합친 전체 데이터셋으로 모델을 훈련시킨다. 그 후 테스트 데이터셋으로 모델 일반화 성능을 검증한다.</p><p>만약 테스트 데이터셋에 나타난 모델 일반화 성능이 검증에서 나타난 일반화 성능보다 많이 나쁘다면. 이는 검증 데이터셋 자체가 애초에 신뢰성이 없었거나(편향 등) 모델이 수정 반복하면서 검증용 셋에 과대적합 된 결과일 수 있다.</p><p>어찌됬건 둘 다 검증용 셋이 신뢰성을 잃어버린 경우다. 따라서 기존 모델 파기하고, 새 모델에 대해 반복 K-겹 교차검증 등을 써서 검증함으로써 검증 과정 신뢰성을 확보해야 한다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a>, <a href='/categories/python/'>python</a>, <a href='/categories/keras/'>Keras</a>, <a href='/categories/deep-learning/'>deep learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/data-science/" class="post-tag no-text-decoration" >data science</a> <a href="/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/tags/keras/" class="post-tag no-text-decoration" >keras</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >deep learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"></div><div class="share-wrapper"> <span class="share-label text-muted mr-1">공유하기</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%5BKeras%2F%EB%94%A5%EB%9F%AC%EB%8B%9D+%EA%B3%B5%EB%B6%80%5D+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EA%B8%B0%EB%B2%95+%EB%B6%84%EB%A5%98%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B+%EB%B6%84%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0+%EC%A0%84%EC%B2%98%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EA%B7%9C%EC%A0%9C+%EA%B8%B0%EB%B2%95%2C+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EC%9E%91%EC%97%85+%ED%9D%90%EB%A6%84+-+der+Wille+zur+Macht&url=https%3A%2F%2Fkibeomkim.me%2Fposts%2Fmachine_learning_basic%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%5BKeras%2F%EB%94%A5%EB%9F%AC%EB%8B%9D+%EA%B3%B5%EB%B6%80%5D+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EA%B8%B0%EB%B2%95+%EB%B6%84%EB%A5%98%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B+%EB%B6%84%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0+%EC%A0%84%EC%B2%98%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EA%B7%9C%EC%A0%9C+%EA%B8%B0%EB%B2%95%2C+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EC%9E%91%EC%97%85+%ED%9D%90%EB%A6%84+-+der+Wille+zur+Macht&u=https%3A%2F%2Fkibeomkim.me%2Fposts%2Fmachine_learning_basic%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fkibeomkim.me%2Fposts%2Fmachine_learning_basic%2F&text=%5BKeras%2F%EB%94%A5%EB%9F%AC%EB%8B%9D+%EA%B3%B5%EB%B6%80%5D+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EA%B8%B0%EB%B2%95+%EB%B6%84%EB%A5%98%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B+%EB%B6%84%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EB%8D%B0%EC%9D%B4%ED%84%B0+%EC%A0%84%EC%B2%98%EB%A6%AC+%EA%B8%B0%EB%B2%95%2C+%EA%B7%9C%EC%A0%9C+%EA%B8%B0%EB%B2%95%2C+%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D+%EC%9E%91%EC%97%85+%ED%9D%90%EB%A6%84+-+der+Wille+zur+Macht" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="링크 복사하기" data-title-succeed="링크가 복사되었습니다!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="tigerkey10/bioinfo.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">최근 업데이트</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/classifier/">[알고리즘/지도학습] 로지스틱 회귀, 서포트벡터 머신, 나이브 베이즈 알고리듬, 분류모형 별 성능 비교</a><li><a href="/posts/regressor/">[알고리즘/지도학습] 회귀문제 - 선형회귀, 의사결정회귀나무, 의사결정회귀나무 앙상블(그래디언트 부스트)</a><li><a href="/posts/boosting/">[알고리즘/지도학습] 앙상블 알고리즘-부스팅(에이다 부스트, 그래디언트 부스트)</a><li><a href="/posts/ensemble/">[알고리즘/지도학습] 앙상블 알고리즘-취합(다수결 투표, 배깅, 랜덤포레스트)</a><li><a href="/posts/decision_tree/">[알고리즘/지도학습] 의사결정나무(Decision Tree) 알고리듬</a></ul></div><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/mathematics/">mathematics</a> <a class="post-tag" href="/tags/data-science/">data science</a> <a class="post-tag" href="/tags/computer-science/">computer science</a> <a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/data-structure/">data structure</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/ml/">ML</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">바로가기</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>관련된 글</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/keras_with_mnist/"><div class="card-body"> <em class="small" data-ts="1642431600" data-df="YYYY-MM-DD" > 2022-01-18 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Keras/딥러닝 공부] 신경망 기본 개념, 텐서, MNIST 데이터 분류하기</h3><div class="text-muted small"><p> 아래 내용은 ‘케라스 창시자에게 배우는 딥러닝 (프랑소와 슐레 저, 박해선 옮김, 길벗 출판사)’ 을 공부한 뒤, 배운 내용을 제 언어로 정리.기록한 것 입니다. 신경망 기본 개념 신경망은 여러 겹의 ‘층(Layer)’ 으로 이루어져 있다. 각 층은 데이터 특징 추출 ‘필터’ 이다. 각 층은 함수로 생각할 수도 있다. 신경망은 여러...</p></div></div></a></div><div class="card"> <a href="/posts/keras_problems/"><div class="card-body"> <em class="small" data-ts="1642950000" data-df="YYYY-MM-DD" > 2022-01-24 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Keras/딥러닝 공부] 신경망 기본 구성, 이진분류, 다중분류, 회귀문제</h3><div class="text-muted small"><p> 아래 내용은 ‘케라스 창시자에게 배우는 딥러닝 (프랑소와 슐레 저, 박해선 옮김, 길벗 출판사)’ 을 공부한 뒤, 배운 내용을 제 언어로 정리.기록한 것 입니다. 신경망 구조 신경망 구성.훈련에 관련된 요소들 층: 모델 기본 구성 단위. 입력값과 정답값(타겟.레이블) 최적화 대상인. 손실함수 최적화 방식. 옵티마이저 층...</p></div></div></a></div><div class="card"> <a href="/posts/cnn/"><div class="card-body"> <em class="small" data-ts="1645974000" data-df="YYYY-MM-DD" > 2022-02-28 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Keras/딥러닝 공부] 합성곱 신경망(CNN) 이론</h3><div class="text-muted small"><p> 아래 내용은 ‘케라스 창시자에게 배우는 딥러닝 (프랑소와 슐레 저, 박해선 옮김, 길벗 출판사)’ 을 공부한 뒤, 배운 내용을 제 언어로 정리.기록한 것 입니다. 합성곱 신경망 기본 합성곱 층과 풀링 층 교차해서 쌓는 게 합성곱 신경망 기본 구조다. [이미지 출처: http://taewan.kim/post/cnn/] 완전 연결 층(De...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/sort_series/" class="btn btn-outline-primary" prompt="이전 글"><p>[2021 인공지능전문가 교육과정 복습] 삽입 정렬, 버블 정렬, 쉘 정렬 알고리듬</p></a> <a href="/posts/heap_merge_sort/" class="btn btn-outline-primary" prompt="다음 글"><p>[2021 인공지능전문가 교육과정 복습] (이진)힙 정렬, 합병 정렬 알고리듬</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/tigerkey10/bioinfo.github.io">Ki Beom Kim</a>.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/study/">study</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/mathematics/">mathematics</a> <a class="post-tag" href="/tags/data-science/">data science</a> <a class="post-tag" href="/tags/computer-science/">computer science</a> <a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/data-structure/">data structure</a> <a class="post-tag" href="/tags/algorithm/">algorithm</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/ml/">ML</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">새 버전의 콘텐츠를 사용할 수 있습니다.</p><button type="button" class="btn btn-primary" aria-label="Update"> 업데이트 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">검색 결과가 없습니다.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
